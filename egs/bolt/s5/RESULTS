# note from Dan: someone please make this RESULTS file up to date, to reflect current results!

# using callhome data only
%WER 65.4 | 6164 43778 | 38.6 44.4 17.0 4.0 65.4 84.2 | -0.615 | tri1/decode_dev/score_12/dev.char.ctm.sys
%WER 65.1 | 6164 43778 | 39.4 45.0 15.6 4.6 65.1 84.3 | -0.737 | tri2/decode_dev/score_11/dev.char.ctm.sys
%WER 63.0 | 6164 43778 | 41.0 42.9 16.1 4.1 63.0 83.4 | -0.642 | tri3a/decode_dev/score_11/dev.char.ctm.sys
%WER 58.7 | 6164 43778 | 45.9 40.5 13.6 4.6 58.7 82.2 | -0.695 | tri4a/decode_dev/score_11/dev.char.ctm.sys
%WER 51.2 | 6164 57276 | 52.3 35.2 12.5 3.5 51.2 82.8 | -0.715 | tri5a/decode_dev/score_10/dev.char.ctm.sys
%WER 44.0 | 6164 57276 | 59.8 31.0 9.2 3.7 44.0 79.7 | -0.592 |  tri6_nnet_gpu/decode_dev/score_8/dev.char.ctm.sys

# using interpolated LM and additional data for AM training:
%WER 34.8 | 6164 57390 | 68.2 23.2 8.5 3.1 34.8 77.2 | -0.323 | exp/tri6_nnet/decode_dev/score_10/dev.char.ctm.sys
%WER 33.3 | 6164 57390 | 70.2 22.8 7.1 3.4 33.3 75.9 | -0.205 | exp/tri6_nnet_mpe/decode_dev_epoch4/score_12/dev.char.ctm.sys

%WER 38.1 | 6164 57390 | 64.9 25.2 9.9 3.0 38.1 79.2 | -0.262 | exp/nnet2_online/nnet_a_gpu_online/decode_dev/score_10/dev.char.ctm.sys
%WER 38.4 | 6164 57390 | 65.0 26.1 8.9 3.4 38.4 80.3 | -0.391 | exp/nnet2_online/nnet_a_gpu_online/decode_dev_utt/score_9/dev.char.ctm.sys
%WER 37.3 | 6164 57390 | 66.1 25.3 8.5 3.4 37.3 79.9 | -0.350 | exp/nnet2_online/nnet_a_gpu_online/decode_dev_utt_offline/score_9/dev.char.ctm.sys
%WER 37.9 | 6164 57390 | 65.0 25.2 9.8 3.0 37.9 79.0 | -0.241 | exp/nnet2_online/nnet_a_gpu/decode_dev/score_10/dev_hires.char.ctm.sys

# from Dan: at /home/dpovey/kaldi-bolt/egs/bolt/s5 I ran the online-nnet2 steup.
# I got the following, which seems quite a bit worse than the tri6_nnet results (assuming we're using
# the same LM).
#for dir in exp/nnet2_online/nnet_a_gpu*/decode_dev*; do grep Sum $dir/score_*/*ys | utils/best_wer.sh; done
#%WER 39.8 | 6164 57390 | 63.5 27.7 8.8 3.3 39.8 79.4 | -0.327 | exp/nnet2_online/nnet_a_gpu_online/decode_dev/score_7/dev.char.ctm.sys
#%WER 40.4 | 6164 57390 | 63.2 28.2 8.7 3.6 40.4 81.4 | -0.388 | exp/nnet2_online/nnet_a_gpu_online/decode_dev_utt/score_7/dev.char.ctm.sys
#%WER 39.3 | 6164 57390 | 63.9 27.0 9.1 3.2 39.3 80.8 | -0.265 | exp/nnet2_online/nnet_a_gpu_online/decode_dev_utt_offline/score_8/dev.char.ctm.sys
#
# There is a lot of overtraining going on:
#grep LOG  exp/nnet2_online/nnet_a_gpu/log/compute_*final.log
#exp/nnet2_online/nnet_a_gpu/log/compute_prob_train.final.log:LOG (nnet-compute-prob:main():nnet-compute-prob.cc:91) Saw 4000 examples, average probability is -1.53914 and accuracy is 0.6285 with total weight 4000
#exp/nnet2_online/nnet_a_gpu/log/compute_prob_valid.final.log:LOG (nnet-compute-prob:main():nnet-compute-prob.cc:91) Saw 4000 examples, average probability is -2.31762 and accuracy is 0.44625 with total weight 4000
# 
# I see that I was mixing up way too much, to 12000, vs. 2887 leaves.
# Probably we shoulo be mixing up to 5000 or so.
# Also, the input feature dimension is probably too much, you can see
# from running nnet-am-info on the final.mdl that the bulk of the
# parameters are in the first layer.
# I think a good solution for this would be to halve the --num-ceps in
# conf/mfc_hires.conf, from 40 to 20, and when you do this you should probably
# modify the num-mel-bins from 40 to 23, which is what we normally use.

# For speed, and since we have overtraining, it might make sense to decrease
# the number of epochs, e.g. from 15 to 8.
